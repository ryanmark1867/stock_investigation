{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marketstack experiment\n",
    "- test the basic marketstack API\n",
    "- ingest the close values for a timeframe\n",
    "- convert the relevant part of the JSON that comes back into a dataframe\n",
    "- convert the date to just yyyy-mm-dd and make that value the index of the dataframe\n",
    "- 3250 - 6 = 3244 CSV files generated (from 3261 unique input tickers)\n",
    "- 17 tickers unaccounted for - captured these in batch_ticker_retry_aug9.csv and rerunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from io import StringIO\n",
    "import yaml\n",
    "from datetime import date\n",
    "import requests\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "\n",
    "# For reading stock data from yahoo\n",
    "#import pandas_datareader as pdr\n",
    "from pandas_datareader.data import DataReader\n",
    "# import yahoo_fin.stock_info as si\n",
    "\n",
    "# For time stamps\n",
    "from datetime import datetime\n",
    "\n",
    "# for LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Input\n",
    "from keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "# access datasets from quandl.com - need to pip install Quandl to use\n",
    "import quandl\n",
    "config_file = 'batch_close_load_config.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory is: C:\\personal\\karma_stocks_2021\\stock_investigation\\notebooks\n",
      "path_to_yaml C:\\personal\\karma_stocks_2021\\stock_investigation\\notebooks\\batch_close_load_config.yml\n"
     ]
    }
   ],
   "source": [
    "# load config file\n",
    "current_path = os.getcwd()\n",
    "print(\"current directory is: \"+current_path)\n",
    "\n",
    "path_to_yaml = os.path.join(current_path, config_file)\n",
    "print(\"path_to_yaml \"+path_to_yaml)\n",
    "try:\n",
    "    with open (path_to_yaml, 'r') as c_file:\n",
    "        config = yaml.safe_load(c_file)\n",
    "except Exception as e:\n",
    "    print('Error reading the config file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config parms\n",
    "access_key = config['general']['marketstack_key']\n",
    "# file containing list of tickers to batch load\n",
    "batch_ticker_list_file = config['files']['batch_ticker_list_file']\n",
    "from_date = config['general']['master_start']\n",
    "to_date = config['general']['master_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw data from marketstack\n",
    "def get_close_data(symbol='AAPL.US', api_token='OeAFFmMliFG5orCUuwAKQ8l4WWFQ67YX', from_date = '2021-07-01', to_date = '2021-07-08'):\n",
    "    session = requests.Session()\n",
    "    print(\"symbol is \",symbol)\n",
    "    print(\"from_date \",from_date)\n",
    "    print(\"to_date \", to_date)\n",
    "    # base_url = 'http://api.marketstack.com/v1/tickers/'+symbol+'/eod'\n",
    "    base_url = 'http://api.marketstack.com/v1/eod'\n",
    "    #url = 'https://eodhistoricaldata.com/api/div/%s' % symbol\n",
    "    params = {'access_key': api_token,'symbols':symbol,'limit':10000,'date_from':from_date,'date_to':to_date}\n",
    "    r = session.get(base_url, params=params)\n",
    "    \n",
    "    ticker_dict = json.loads(r.text)[\"data\"]\n",
    "    # print(\"ticker_dict is \", ticker_dict)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        #print(\"status OK \")\n",
    "        df = pd.DataFrame.from_dict(ticker_dict, orient='columns')\n",
    "        #df = pd.read_csv(StringIO(r.text), skipfooter=0, parse_dates=[0], index_col=0, engine='python')\n",
    "        return(True, df)\n",
    "    else:\n",
    "        print(\"status code\",str(r.status_code))\n",
    "        print(\"reason code\",str(r.reason))\n",
    "        return(False,\"null\")\n",
    "        #raise Exception(r.status_code, r.reason, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up dataframe returned by marketstack\n",
    "def ms_df_cleanup(df):\n",
    "    # keep just the yyyy-mm-dd portion of date column\n",
    "    df['date'] = df['date'].str[0:10]\n",
    "    # rename the column to be consistent\n",
    "    df.rename(columns = {'date': 'Date','close': 'Close'}, inplace = True)\n",
    "    # set Date to be the index\n",
    "    df.set_index('Date', inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_path(directory):\n",
    "    '''get the fully qualified path for a peer directory of the directory where this notebook is run'''\n",
    "    rawpath = os.getcwd()\n",
    "    # \"directory\" is in a directory is a sibling to the directory containing the notebook\n",
    "    path = os.path.abspath(os.path.join(rawpath, '..', directory))\n",
    "    return(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name is C:\\personal\\karma_stocks_2021\\stock_investigation\\data\\batch_ticker_retry_vix_urth.csv\n",
      "from date  1991-01-01\n",
      "to date  2021-08-06\n",
      "ticker is  IXIC\n",
      "symbol is  IXIC\n",
      "from_date  1991-01-01\n",
      "to_date  2021-08-06\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-190-e6783ef01daa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#    print(\" i is \",i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ticker is \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_close_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccess_key\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfrom_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mms_df_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mticker_file_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfrom_date\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mto_date\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-187-9729b09a9334>\u001b[0m in \u001b[0;36mget_close_data\u001b[1;34m(symbol, api_token, from_date, to_date)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mticker_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m# print(\"ticker_dict is \", ticker_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "batch_file_name = os.path.join(get_directory_path('data'), batch_ticker_list_file)\n",
    "print(\"file name is\", batch_file_name)\n",
    "print(\"from date \",from_date)\n",
    "print(\"to date \", to_date)\n",
    "df_batch = pd.read_csv(batch_file_name)\n",
    "i = 0\n",
    "i_max = 10\n",
    "for ticker in df_batch['Ticker']:\n",
    "#    if i > i_max:\n",
    "#        break\n",
    "#    i = i+1\n",
    "#    print(\" i is \",i)\n",
    "    print(\"ticker is \",ticker)\n",
    "    result, df_out = get_close_data(ticker,access_key,from_date, to_date)\n",
    "    df = ms_df_cleanup(df_out)\n",
    "    ticker_file_name = ticker+'_'+from_date+'_'+to_date+'.csv'\n",
    "    ticker_batch_file = os.path.join(get_directory_path('static_load'), ticker_file_name)\n",
    "    df.to_csv(ticker_batch_file)\n",
    "print(\"through main loop\")\n",
    "'''\n",
    "result, df_out = get_close_data('aapl',access_key,from_date, to_date)\n",
    "df = ms_df_cleanup(df_out)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
