{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InRCaIfsttEW"
   },
   "source": [
    "# Stock Analysis Using LSTM - MVP\n",
    "- train an LSTM on a variable number of features\n",
    "- vary on lookback (LSTM window) and training window (years before present date)\n",
    "- multi input version\n",
    "- multi day lookahead (predict x days ahead) version\n",
    "\n",
    "This notebook adapted from https://www.kaggle.com/faressayah/stock-market-analysis-prediction-using-lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GA1ksHAIttEf"
   },
   "source": [
    "# Import required libraries and set up notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "executionInfo": {
     "elapsed": 1460,
     "status": "ok",
     "timestamp": 1611806431398,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "GdtAhwhattEh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "\n",
    "# For reading stock data from yahoo\n",
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "# For time stamps\n",
    "from datetime import datetime\n",
    "\n",
    "# for LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Input\n",
    "from keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "# access datasets from quandl.com - need to pip install Quandl to use\n",
    "import quandl\n",
    "on_colab = False\n",
    "verboseout = True\n",
    "look_back = 3\n",
    "config_file = 'model_training_config.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "if on_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "if on_colab:\n",
    "    %cd /content/drive/MyDrive/karma_jan_2021/stock_investigation/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2330,
     "status": "ok",
     "timestamp": 1611806432280,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "uwJ6aCMDttEi",
    "outputId": "8105b877-ad16-4665-e584-59858f3f1108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory is: C:\\personal\\karma_stocks_2021\\stock_investigation\\notebooks\n",
      "path_to_yaml C:\\personal\\karma_stocks_2021\\stock_investigation\\notebooks\\model_training_config.yml\n"
     ]
    }
   ],
   "source": [
    "# load config file\n",
    "current_path = os.getcwd()\n",
    "print(\"current directory is: \"+current_path)\n",
    "\n",
    "path_to_yaml = os.path.join(current_path, config_file)\n",
    "print(\"path_to_yaml \"+path_to_yaml)\n",
    "try:\n",
    "    with open (path_to_yaml, 'r') as c_file:\n",
    "        config = yaml.safe_load(c_file)\n",
    "except Exception as e:\n",
    "    print('Error reading the config file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "executionInfo": {
     "elapsed": 2324,
     "status": "ok",
     "timestamp": 1611806432284,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "6Q2YaYEmttEj"
   },
   "outputs": [],
   "source": [
    "# switches\n",
    "plot_all = config['general']['verboseout']\n",
    "use_saved_model = config['general']['presaved']\n",
    "ust_cols = config['general']['ust_cols']\n",
    "# list of ticket symbols\n",
    "# e.g. tech_list = ['AAPL', 'GOOG', 'MSFT', 'AMZN']\n",
    "tech_list = config['tech_list']\n",
    "# dictionary to seed stock handles with company names\n",
    "# e.g. company_dict = {'AAPL':\"APPLE\", 'GOOG':\"GOOGLE\", 'MSFT':\"MICROSOFT\", 'AMZN':\"AMAZON\"}\n",
    "company_dict = config['company_dict']\n",
    "saved_model_modifier = config['files']['saved_model_modifier']\n",
    "# get the column lists\n",
    "continuouscols = config['categorical']\n",
    "textcols = config['text']\n",
    "collist = config['continuous']\n",
    "years_window = config['general']['years_window']\n",
    "quandl_token = config['general']['quandl_token']\n",
    "# number of days previous used to predict subsequent day\n",
    "look_back = config['general']['look_back']\n",
    "# gap in day betwen the last day previous and the subsequent day being predicted\n",
    "look_ahead = config['general']['look_ahead']\n",
    "years_winow = config['general']['years_window']\n",
    "# target column\n",
    "lstm_target = config['general']['lstm_target']\n",
    "# list from target column\n",
    "lstm_target_list = [lstm_target]\n",
    "# additional features beyond the target column\n",
    "feature_list = config['general']['feature_list']\n",
    "# complete list of features LSTM trained on\n",
    "lstm_feature_list = lstm_target_list+feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2297,
     "status": "ok",
     "timestamp": 1611806432297,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "XQjAr633CmYT",
    "outputId": "82a71416-6d92-4e33-d566-19dbc9298eed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'general': {'verboseout': False,\n",
       "  'includetext': True,\n",
       "  'on_colab': False,\n",
       "  'presaved': False,\n",
       "  'savemodel': False,\n",
       "  'picklemodel': True,\n",
       "  'save_model_plot': False,\n",
       "  'tensorboard_callback': False,\n",
       "  'hctextmax': 7000,\n",
       "  'maxwords': 6000,\n",
       "  'textmax': 50,\n",
       "  'pickled_data_file': '20142018_0930.pkl',\n",
       "  'pickled_dataframe': 'AB_NYC_2019_output_aug19_2020.pkl',\n",
       "  'modifier': 'oct05_2020',\n",
       "  'targetthresh': 6.0,\n",
       "  'targetcontinuous': False,\n",
       "  'target_col': 'price',\n",
       "  'emptythresh': 6000,\n",
       "  'zero_weight': 1.0,\n",
       "  'one_weight': 45.878,\n",
       "  'one_weight_offset': 0,\n",
       "  'patience_threshold': 3,\n",
       "  'master_start': '2018-01-01',\n",
       "  'ust_cols': ['2 YR'],\n",
       "  'years_window': 2,\n",
       "  'quandl_token': 'uCghYBw8CtpUvWct_W8c',\n",
       "  'look_back': 3,\n",
       "  'look_ahead': 14,\n",
       "  'lstm_target': 'Close',\n",
       "  'feature_list': ['2 YR', '30 YR']},\n",
       " 'files': {'saved_model_modifier': 'January312021'},\n",
       " 'tech_list': ['CROX'],\n",
       " 'company_dict': {'CROX': 'Crocs'},\n",
       " 'test_parms': {'testproportion': 0.2,\n",
       "  'trainproportion': 0.8,\n",
       "  'current_experiment': 5,\n",
       "  'repeatable_run': False,\n",
       "  'get_test_train_acc': True},\n",
       " 'categorical': ['neighbourhood_group', 'neighbourhood', 'room_type'],\n",
       " 'continuous': ['minimum_nights',\n",
       "  'number_of_reviews',\n",
       "  'reviews_per_month',\n",
       "  'calculated_host_listings_count'],\n",
       " 'text': [],\n",
       " 'excluded': ['price',\n",
       "  'id',\n",
       "  'latitude',\n",
       "  'longitude',\n",
       "  'host_name',\n",
       "  'last_review',\n",
       "  'name',\n",
       "  'host_name',\n",
       "  'availability_365'],\n",
       " 'hyperparameters': {'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0003,\n",
       "  'l2_lambda': 0.0003,\n",
       "  'loss_func': 'binary_crossentropy',\n",
       "  'output_activation': 'hard_sigmoid',\n",
       "  'batch_size': 1000,\n",
       "  'epochs': 50}}"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load US Treasury stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load US Treasury yield dataframe\n",
    "ust_df = quandl.get(\"USTREASURY/YIELD\", authtoken=quandl_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 MO</th>\n",
       "      <th>2 MO</th>\n",
       "      <th>3 MO</th>\n",
       "      <th>6 MO</th>\n",
       "      <th>1 YR</th>\n",
       "      <th>2 YR</th>\n",
       "      <th>3 YR</th>\n",
       "      <th>5 YR</th>\n",
       "      <th>7 YR</th>\n",
       "      <th>10 YR</th>\n",
       "      <th>20 YR</th>\n",
       "      <th>30 YR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.89</td>\n",
       "      <td>7.81</td>\n",
       "      <td>7.87</td>\n",
       "      <td>7.90</td>\n",
       "      <td>7.87</td>\n",
       "      <td>7.98</td>\n",
       "      <td>7.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.89</td>\n",
       "      <td>7.94</td>\n",
       "      <td>7.85</td>\n",
       "      <td>7.94</td>\n",
       "      <td>7.96</td>\n",
       "      <td>7.92</td>\n",
       "      <td>8.04</td>\n",
       "      <td>7.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.90</td>\n",
       "      <td>7.82</td>\n",
       "      <td>7.92</td>\n",
       "      <td>7.93</td>\n",
       "      <td>7.91</td>\n",
       "      <td>8.02</td>\n",
       "      <td>7.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.79</td>\n",
       "      <td>7.85</td>\n",
       "      <td>7.79</td>\n",
       "      <td>7.90</td>\n",
       "      <td>7.94</td>\n",
       "      <td>7.92</td>\n",
       "      <td>8.03</td>\n",
       "      <td>7.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990-01-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.79</td>\n",
       "      <td>7.88</td>\n",
       "      <td>7.81</td>\n",
       "      <td>7.90</td>\n",
       "      <td>7.95</td>\n",
       "      <td>7.92</td>\n",
       "      <td>8.05</td>\n",
       "      <td>8.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-02-12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-02-16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-02-17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.91</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-02-19</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7789 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1 MO  2 MO  3 MO  6 MO  1 YR  2 YR  3 YR  5 YR  7 YR  10 YR  \\\n",
       "Date                                                                      \n",
       "1990-01-02   NaN   NaN  7.83  7.89  7.81  7.87  7.90  7.87  7.98   7.94   \n",
       "1990-01-03   NaN   NaN  7.89  7.94  7.85  7.94  7.96  7.92  8.04   7.99   \n",
       "1990-01-04   NaN   NaN  7.84  7.90  7.82  7.92  7.93  7.91  8.02   7.98   \n",
       "1990-01-05   NaN   NaN  7.79  7.85  7.79  7.90  7.94  7.92  8.03   7.99   \n",
       "1990-01-08   NaN   NaN  7.79  7.88  7.81  7.90  7.95  7.92  8.05   8.02   \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...    ...   \n",
       "2021-02-12  0.03  0.04  0.04  0.05  0.06  0.11  0.20  0.50  0.85   1.20   \n",
       "2021-02-16  0.03  0.04  0.04  0.06  0.08  0.13  0.23  0.57  0.94   1.30   \n",
       "2021-02-17  0.03  0.04  0.04  0.06  0.07  0.11  0.21  0.57  0.94   1.29   \n",
       "2021-02-18  0.03  0.03  0.03  0.04  0.06  0.11  0.21  0.56  0.94   1.29   \n",
       "2021-02-19  0.03  0.03  0.04  0.06  0.07  0.11  0.22  0.59  0.98   1.34   \n",
       "\n",
       "            20 YR  30 YR  \n",
       "Date                      \n",
       "1990-01-02    NaN   8.00  \n",
       "1990-01-03    NaN   8.04  \n",
       "1990-01-04    NaN   8.04  \n",
       "1990-01-05    NaN   8.06  \n",
       "1990-01-08    NaN   8.09  \n",
       "...           ...    ...  \n",
       "2021-02-12   1.83   2.01  \n",
       "2021-02-16   1.92   2.08  \n",
       "2021-02-17   1.92   2.06  \n",
       "2021-02-18   1.91   2.08  \n",
       "2021-02-19   1.98   2.14  \n",
       "\n",
       "[7789 rows x 12 columns]"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ust_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2 YR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>7.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990-01-03</td>\n",
       "      <td>7.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990-01-04</td>\n",
       "      <td>7.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990-01-05</td>\n",
       "      <td>7.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990-01-08</td>\n",
       "      <td>7.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            2 YR\n",
       "Date            \n",
       "1990-01-02  7.87\n",
       "1990-01-03  7.94\n",
       "1990-01-04  7.92\n",
       "1990-01-05  7.90\n",
       "1990-01-08  7.90"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new = old[['A', 'C', 'D']].copy()\n",
    "ust_df_2year = ust_df[['2 YR']].copy()\n",
    "ust_df_2year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 2755,
     "status": "ok",
     "timestamp": 1611806432764,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "aHWsIsOlttEk",
    "outputId": "6c74987f-3eaf-4bbb-b075-aac34a58e469"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>43.092499</td>\n",
       "      <td>42.575001</td>\n",
       "      <td>42.950001</td>\n",
       "      <td>42.764999</td>\n",
       "      <td>68998800.0</td>\n",
       "      <td>41.887783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-02-22</td>\n",
       "      <td>43.250000</td>\n",
       "      <td>42.845001</td>\n",
       "      <td>42.895000</td>\n",
       "      <td>43.242500</td>\n",
       "      <td>75652800.0</td>\n",
       "      <td>42.355495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-02-25</td>\n",
       "      <td>43.967499</td>\n",
       "      <td>43.487499</td>\n",
       "      <td>43.540001</td>\n",
       "      <td>43.557499</td>\n",
       "      <td>87493600.0</td>\n",
       "      <td>42.664028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-02-26</td>\n",
       "      <td>43.825001</td>\n",
       "      <td>43.292500</td>\n",
       "      <td>43.427502</td>\n",
       "      <td>43.582500</td>\n",
       "      <td>68280800.0</td>\n",
       "      <td>42.688511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-02-27</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>43.182499</td>\n",
       "      <td>43.302502</td>\n",
       "      <td>43.717499</td>\n",
       "      <td>111341600.0</td>\n",
       "      <td>42.820747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 High        Low       Open      Close       Volume  Adj Close\n",
       "Date                                                                          \n",
       "2019-02-21  43.092499  42.575001  42.950001  42.764999   68998800.0  41.887783\n",
       "2019-02-22  43.250000  42.845001  42.895000  43.242500   75652800.0  42.355495\n",
       "2019-02-25  43.967499  43.487499  43.540001  43.557499   87493600.0  42.664028\n",
       "2019-02-26  43.825001  43.292500  43.427502  43.582500   68280800.0  42.688511\n",
       "2019-02-27  43.750000  43.182499  43.302502  43.717499  111341600.0  42.820747"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataReader: https://riptutorial.com/pandas/topic/1912/pandas-datareader\n",
    "# https://pandas-datareader.readthedocs.io/en/latest/remote_data.html\n",
    "# \n",
    "# Set up End and Start times for data grab\n",
    "end = datetime.now()\n",
    "start = datetime(end.year - years_window, end.month, end.day)\n",
    "tester = DataReader('AAPL', 'yahoo', start, end)\n",
    "tester.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "executionInfo": {
     "elapsed": 3901,
     "status": "ok",
     "timestamp": 1611806433919,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "DVAQQ2CAttEl",
    "outputId": "0e3265fb-d6c3-4371-d196-be3d9a8924db"
   },
   "outputs": [],
   "source": [
    "# The tech stocks used for this analysis\n",
    "#tech_list = ['AAPL', 'GOOG', 'MSFT', 'AMZN']\n",
    "tech_list = config['tech_list']\n",
    "# dictionary to seed stock handles with company names\n",
    "#company_dict = {'AAPL':\"APPLE\", 'GOOG':\"GOOGLE\", 'MSFT':\"MICROSOFT\", 'AMZN':\"AMAZON\"}\n",
    "company_dict = config['company_dict']\n",
    "\n",
    "# Set up End and Start times for data grab\n",
    "end = datetime.now()\n",
    "start = datetime(end.year - years_window, end.month, end.day)\n",
    "\n",
    "stock_dict = {}\n",
    "#For loop for grabing yahoo finance data and setting as a dataframe\n",
    "# TODO correct use of globals() here\n",
    "seeded = False\n",
    "for stock in tech_list:   \n",
    "    # Set DataFrame as the Stock Ticker\n",
    "    stock_dict[stock] = DataReader(stock, 'yahoo', start, end)\n",
    "    # add column for company name\n",
    "    stock_dict[stock]['company_name'] = company_dict[stock]\n",
    "    # build up overall dataframe containing stock info for all companies\n",
    "    if seeded:\n",
    "        df = pd.concat([df,stock_dict[stock]],axis=0)\n",
    "    else:\n",
    "        seeded = True\n",
    "        df = stock_dict[stock]\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 3894,
     "status": "ok",
     "timestamp": 1611806433922,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "velBocHnHsag",
    "outputId": "649289a4-e83a-40f0-e690-ab11132d71e2"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nH94b3sPttEl"
   },
   "source": [
    "Let's go ahead and play aorund with the AAPL DataFrame to get a feel for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VY3r0v1ttEm"
   },
   "source": [
    "# Create dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCa3-nUpttEp"
   },
   "source": [
    "# 2. What was the moving average of the various stocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3865,
     "status": "ok",
     "timestamp": 1611806433932,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "l6AqBdUZttEp"
   },
   "outputs": [],
   "source": [
    "ma_day = [10, 20, 50]\n",
    "\n",
    "for ma in ma_day:\n",
    "    for company in tech_list:\n",
    "        column_name = f\"MA for {ma} days\"\n",
    "        stock_dict[company][column_name] = stock_dict[company]['Adj Close'].rolling(ma).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEgA2qjYttE2"
   },
   "source": [
    "# 6. Predicting closing stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11046,
     "status": "ok",
     "timestamp": 1611806441227,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "Q39q1pn_ttE2"
   },
   "outputs": [],
   "source": [
    "#Get the stock quote\n",
    "# tech_list = ['AAPL', 'GOOG', 'MSFT', 'AMZN']\n",
    "df_predict = {}\n",
    "# start='2012-01-01'\n",
    "for company in tech_list:\n",
    "    df_predict[company] = DataReader(company,data_source='yahoo', start=start, end=datetime.now())\n",
    "    # check if anything being joined\n",
    "    if len(feature_list) > 0:\n",
    "        df_predict[company] = pd.merge(df_predict[company],ust_df[feature_list],on=['Date'],how='inner')\n",
    "    # add\n",
    "    # show an example\n",
    "df_predict[tech_list[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11036,
     "status": "ok",
     "timestamp": 1611806441230,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "ceD3yTC4ttE3"
   },
   "outputs": [],
   "source": [
    "def get_model_path():\n",
    "    '''get the path for data files'''\n",
    "    rawpath = os.getcwd()\n",
    "    # data is in a directory called \"data\" that is a sibling to the directory containing the notebook\n",
    "    path = os.path.abspath(os.path.join(rawpath, '..', 'models'))\n",
    "    return(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11032,
     "status": "ok",
     "timestamp": 1611806441231,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "SSrv9zAhttE3"
   },
   "outputs": [],
   "source": [
    "def save_model(model,company):\n",
    "    '''save the model to a file in directory models that is a peer of the directory containing this notebook'''\n",
    "    model_path = get_model_path()\n",
    "    modifier = datetime.now().strftime(\"%B%d%Y\")\n",
    "    save_model_path = os.path.join(model_path,company+modifier+'.h5')\n",
    "    model.save(save_model_path)\n",
    "    # no early stop, so make current model saved_model\n",
    "    print(\"Saved model with name \",company+modifier+'.h5')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11028,
     "status": "ok",
     "timestamp": 1611806441234,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "ijCLNQ1LttE3"
   },
   "outputs": [],
   "source": [
    "def get_save_model_path(company):\n",
    "    model_path = get_model_path()\n",
    "    modifier = saved_model_modifier\n",
    "    # modifier = datetime.now().strftime(\"%B%d%Y\")\n",
    "    save_model_path = os.path.join(model_path,company+modifier+'.h5')\n",
    "    return(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11023,
     "status": "ok",
     "timestamp": 1611806441236,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "qS4bPikXttE3"
   },
   "outputs": [],
   "source": [
    "def one_col_df(df,col):\n",
    "    data = df.filter([col])\n",
    "    dataset = data.values\n",
    "    training_data_len = int(np.ceil( len(dataset) * .8 ))\n",
    "    return(data,dataset,training_data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_col_df(df,cols):\n",
    "    print(\"cols is:\", cols)\n",
    "    #data = df.filter(cols)\n",
    "    data = df[cols]\n",
    "    #print(\"data inside is:\",data)\n",
    "    dataset = data.values\n",
    "    training_data_len = int(np.ceil( len(dataset) * .8 ))\n",
    "    return(data,dataset,training_data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_vars(dataset):\n",
    "    '''cribbed from traditional DL on structured data code'''\n",
    "    X = {}\n",
    "    dictlist = []\n",
    "    for col in collist:\n",
    "        if verboseout:\n",
    "            print(\"cat col is\",col)\n",
    "        X[col] = np.array(dataset[col])\n",
    "        dictlist.append(np.array(dataset[col]))\n",
    "       \n",
    "    for col in textcols:\n",
    "        if verboseout:\n",
    "            print(\"text col is\",col)\n",
    "        X[col] = pad_sequences(dataset[col], maxlen=max_dict[col])\n",
    "        dictlist.append(pad_sequences(dataset[col], maxlen=max_dict[col]))\n",
    "        \n",
    "    for col in continuouscols:\n",
    "        if verboseout:\n",
    "            print(\"cont col is\",col)\n",
    "        X[col] = np.array(dataset[col])\n",
    "        dictlist.append(np.array(dataset[col]))\n",
    "        \n",
    "    return X, dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11017,
     "status": "ok",
     "timestamp": 1611806441237,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "6bfV2DBrttE4"
   },
   "outputs": [],
   "source": [
    "def scale_data(dataset):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "    return(scaled_data,scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11009,
     "status": "ok",
     "timestamp": 1611806441239,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "ERZT1IH6ttE4"
   },
   "outputs": [],
   "source": [
    "def create_x_y(scaled_data,training_data_len,look_back,input_col_count):\n",
    "    '''convert scaled dataset into numpy x and y arrays for training'''\n",
    "    #Create the training data set\n",
    "    #Create the scaled training data set\n",
    "    print(\"scaled_data shape: \",str(scaled_data.shape))\n",
    "    print('training_data_len: ',training_data_len)\n",
    "    train_data = scaled_data[0:int(training_data_len), :]\n",
    "    print(\"train_data shape is: \",train_data.shape)\n",
    "    #Split the data into x_train and y_train data sets\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    print(\"len(train_data): \",str(len(train_data)))\n",
    "    # here, each x is a set of look_back values and each y is the next value\n",
    "    # a = dataset[i:(i + look_back), :]\n",
    "    for i in range(len(train_data) - look_back - look_ahead - 1):\n",
    "        a = train_data[i:(i + look_back), :]\n",
    "        x_train.append(a)\n",
    "        y_train.append(train_data[i+look_ahead, 0])\n",
    "        \n",
    "\n",
    "    # Convert the x_train and y_train to numpy arrays \n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "    #Reshape the data\n",
    "    #x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    # x_train.shape\n",
    "    return(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train,y_train,optimizer,loss_function):\n",
    "        '''build simple model given x, y, optimizer and loss function - sequential API'''\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n",
    "        model.add(LSTM(64, return_sequences=False))\n",
    "        model.add(Dense(25))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer, loss=loss_function)\n",
    "\n",
    "        #Train the model\n",
    "       \n",
    "        model.fit(x_train, y_train, batch_size=1, epochs=1)\n",
    "        return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_func(x_train,y_train,optimizer,loss_function,input_dim):\n",
    "        '''build simple model given x, y, optimizer and loss function - functional API'''\n",
    "        '''reference https://stats.stackexchange.com/questions/274478/understanding-input-shape-parameter-in-lstm-with-keras\n",
    "            and https://stackoverflow.com/questions/42532386/how-to-work-with-multiple-inputs-for-lstm-in-keras'''\n",
    "        lstminputs = {}\n",
    "        inputlayerlist = []\n",
    "        col = \"holder\"\n",
    "        #lstminputs[col] = Input(shape=(x_train.shape[1],1),name=col)\n",
    "        lstminputs[col] = Input(shape=(x_train.shape[1],input_dim),name=col)\n",
    "        inputlayerlist.append(lstminputs[col])\n",
    "        lstminputs[col] = (LSTM(128,return_sequences=True) (lstminputs[col]))\n",
    "        lstminputs[col] = (LSTM(64,return_sequences=False) (lstminputs[col]))\n",
    "        lstminputs[col] = (Dense(25) (lstminputs[col]))\n",
    "        \n",
    "        #output = Dense(1, activation=output_activation) (lstminputs[col])\n",
    "        output = Dense(1) (lstminputs[col])\n",
    "        # define model\n",
    "\n",
    "        model = Model(inputlayerlist, output)\n",
    "\n",
    "        \n",
    "        \n",
    " \n",
    "        model.compile(optimizer=optimizer, loss=loss_function)\n",
    "\n",
    "        #Train the model\n",
    "       \n",
    "        model.fit(x_train, y_train, batch_size=1, epochs=1)\n",
    "        return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_func2(x_train,y_train,optimizer,loss_function,collist,continuouscols,lstmcols):\n",
    "    '''build multi-input functional API model\n",
    "    collist - list of categorical columns\n",
    "    continuouscols - list of continuous columns\n",
    "    \n",
    "    want to have x_train being a list of numpy arrays, \n",
    "    one for each column that we want to create layers for\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    catinputs = {} # list of categorical inputs\n",
    "    textinputs = {} # list of text inputs\n",
    "    lstminput = {} # list of lstm inputs\n",
    "    continputs = {} # list of continuous inputs\n",
    "    embeddings = {}\n",
    "    textembeddings = {}\n",
    "    catemb = 10 # size of categorical embeddings\n",
    "    textemb = 50 # size of text embeddings\n",
    "    inputlayerlist = []\n",
    "    \n",
    "\n",
    "    \n",
    "    '''\n",
    "    for col in lstmcols:\n",
    "        lstminputs[col] = Input(shape=(x_train.shape[1],1),name=col)\n",
    "        inputlayerlist.append(lstminputs[col])\n",
    "        lstminputs[col] = (LSTM(128,return_sequences=True) (lstminputs[col]))\n",
    "        lstminputs[col] = (LSTM(64,return_sequences=False) (lstminputs[col]))\n",
    "        lstminputs[col] = (Dense(25) (lstminputs[col]))\n",
    "        # question here whether to include a dense layer or not at the end of LSTM \n",
    "        '''\n",
    "    for col in collist:\n",
    "        catinputs[col] = Input(shape=[1],name=col)\n",
    "        inputlayerlist.append(catinputs[col])\n",
    "        #print(\"inputname\",inputname)\n",
    "        embeddings[col] = (Embedding(max_dict[col],catemb) (catinputs[col]))\n",
    "        # batchnorm all\n",
    "        embeddings[col] = (BatchNormalization() (embeddings[col]))\n",
    "        collistfix.append(embeddings[col])\n",
    "    for col in continuouscols:\n",
    "        continputs[col] = Input(shape=[1],name=col)\n",
    "        inputlayerlist.append(continputs[col])\n",
    "        \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/53049396/sklearn-inverse-transform-return-only-one-column-when-fit-to-many\n",
    "def invTransform(scaler, data, colName, colNames):\n",
    "    # wrapper function to get inverse transform on one column from a dataframe that has been scaled\n",
    "    # - scaler   = the scaler object (it needs an inverse_transform method)\n",
    "    # - data     = the data to be inverse transformed as a Series, ndarray, ... \n",
    "    #              (a 1d object you can assign to a df column)\n",
    "    # - ftName   = the name of the column to which the data belongs\n",
    "    # - colNames = all column names of the data on which scaler was fit \n",
    "    #              (necessary because scaler will only accept a df of the same shape as the one it was fit on)\n",
    "    dummy = pd.DataFrame(np.zeros((len(data), len(colNames))), columns=colNames)\n",
    "    dummy[colName] = data\n",
    "    dummy = pd.DataFrame(scaler.inverse_transform(dummy), columns=colNames)\n",
    "    return dummy[colName].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29702,
     "status": "ok",
     "timestamp": 1611806459942,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "yU3zPKDZttE4",
    "outputId": "dce1cfd6-efac-439d-b311-ff58364c5f83",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train models for each company in tech_list and save trained models\n",
    "predictions = {}\n",
    "data = {}\n",
    "for company in tech_list:\n",
    "    #data[company],dataset,training_data_len = one_col_df(df_predict[company],'Close')\n",
    "    data[company],dataset,training_data_len = multi_col_df(df_predict[company],lstm_feature_list)\n",
    "    # X, dictlist get_keras_vars(df_predict[company])\n",
    "    #print(\"dataset is: \",dataset)\n",
    "    #print((\"data[company] is:\",data[company]))\n",
    "    print(\"training_data_len: \",training_data_len)\n",
    "    print(\"dataset shape: \",str(dataset.shape))\n",
    "    print(\"data[company] shape: \",str(data[company].shape))\n",
    "    #scaled_data, scaler = scale_data(dataset)\n",
    "    scaled_data, scaler = scale_data(data[company])\n",
    "    x_train, y_train = create_x_y(scaled_data,training_data_len,look_back,len(lstm_feature_list))\n",
    "    #print(\"x_train is: \",x_train)\n",
    "    #print(\"y_train is: \",y_train)\n",
    "    print(\"x_train shape: \",x_train.shape)\n",
    "    print(\"y_train shape: \",y_train.shape)\n",
    "    print(\"TRAINING LOOP FOR: \",str(company))\n",
    "    '''\n",
    "    turn off for experiment with multi columns\n",
    "    '''\n",
    "    if use_saved_model:\n",
    "        # put here\n",
    "        save_model_path = get_save_model_path(company)\n",
    "        model = load_model(save_model_path)\n",
    "    else:\n",
    "        # train model fresh\n",
    "        #Build the LSTM model\n",
    "        model = build_model_func(x_train,y_train,'adam','mean_squared_error',len(lstm_feature_list))\n",
    "        # save the model\n",
    "        save_model(model,company)\n",
    "    #Create a new array containing scaled values from index 1543 to 2002 \n",
    "    \n",
    "    test_data = scaled_data[training_data_len - look_back: , :]\n",
    "    #Create the data sets x_test and y_test\n",
    "    print(\"about to run test loop\")\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    # y_test = dataset[training_data_len:, :]\n",
    "    for i in range(len(test_data)-look_ahead):\n",
    "        y_test.append(test_data[i+look_ahead, 0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"look_back is\", look_back)\n",
    "    print(\"shape of test_data is \",str(test_data.shape))\n",
    "    print(\"len(test_data) is \",len(test_data))\n",
    "    for i in range(look_back,len(test_data)):\n",
    "        a = test_data[i-look_back:i, :]\n",
    "        x_test.append(a)\n",
    "    print(\"shape of a is \",str(a.shape))\n",
    "    print(\"after test loop\")\n",
    "    '''\n",
    "    for i in range(60, len(test_data)):\n",
    "        x_test.append(test_data[i-60:i, 0])\n",
    "    '''\n",
    "    # Convert the data to a numpy array\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    print(\"x_test shape\",x_test.shape)\n",
    "    print(\"y_test shape\",y_test.shape)\n",
    "    # Reshape the data\n",
    "    #x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "\n",
    "    # Get the models predicted price values \n",
    "    predictions_temp = model.predict(x_test)\n",
    "    print(\"after predictions_temp\")\n",
    "    print(\"predictions temp shape \",predictions_temp.shape)\n",
    "    #print(\"prediction_temp: \",predictions_temp)\n",
    "    # invTransform(scaler, data, colName, colNames)\n",
    "    # predictions[company] = scaler.inverse_transform(predictions_temp)\n",
    "    predictions[company] = invTransform(scaler,predictions_temp,lstm_target,lstm_feature_list)\n",
    "    print(\"predictions company shape \",predictions[company].shape)\n",
    "    predictions[company] = np.reshape(predictions[company], (predictions[company].shape[0],1))\n",
    "    print(\"predictions company shape 2 \",predictions[company].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 31172,
     "status": "ok",
     "timestamp": 1611806461427,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "HcyoiMipttE5",
    "outputId": "4c45f63e-3fc7-458e-88d0-3f958deba6f5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "\n",
    "for company in tech_list:\n",
    "    train = data[company][:training_data_len]\n",
    "    #valid = data[company][training_data_len:]\n",
    "    valid = data[company][training_data_len+look_ahead:]\n",
    "    #train = np.reshape(train,(train.shape[0],1))\n",
    "    #valid = np.reshape(valid,(valid.shape[0],1))\n",
    "    print(\"train shape \",train.shape)\n",
    "    print(\"valid shape \",valid.shape)\n",
    "    print(\"predictions[company] shape \",predictions[company].shape)\n",
    "    print(\"things you want \",str(predictions[company].shape[0]-look_ahead))\n",
    "    valid['Predictions'] = predictions[company][:predictions[company].shape[0]-look_ahead]\n",
    "    # Visualize the data\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('Model'+\" \"+company)\n",
    "    plt.xlabel('Date', fontsize=18)\n",
    "    plt.ylabel(lstm_target+' Price USD ($)', fontsize=18)\n",
    "    plt.plot(train[lstm_target])\n",
    "    plt.plot(valid[[lstm_target, 'Predictions']])\n",
    "    plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
    "    plt.show()\n",
    "print(\"LSTM target is: \",lstm_target)\n",
    "print(\"LSTM features are: \",lstm_feature_list)\n",
    "print(\"lookback is: \",str(look_back))\n",
    "print(\"lookahead is: \",str(look_ahead))\n",
    "print(\"years_window is: \",str(years_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31138,
     "status": "ok",
     "timestamp": 1611806461431,
     "user": {
      "displayName": "Mark Ryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOaJEeoxteIdEhraqpv8y7ol-feJVt-BYY9ceTIQ=s64",
      "userId": "08045617267833954278"
     },
     "user_tz": 300
    },
    "id": "H4wln8eKttE5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nXsLh7jMttEr"
   ],
   "name": "stock_analysis_using_lstm_mvp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
