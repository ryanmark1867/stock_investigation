{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Fundamentals ingest\n",
    "\n",
    "- bring JSON files into dataframes experiment\n",
    "- test to see what we can get from the free level subscription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from io import StringIO\n",
    "import yaml\n",
    "from datetime import date\n",
    "import requests\n",
    "from sklearn.metrics import classification_report\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "\n",
    "# For reading stock data from yahoo\n",
    "#import pandas_datareader as pdr\n",
    "from pandas_datareader.data import DataReader\n",
    "# import yahoo_fin.stock_info as si\n",
    "\n",
    "# For time stamps\n",
    "from datetime import datetime\n",
    "\n",
    "# for LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Input\n",
    "from keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "# access datasets from quandl.com - need to pip install Quandl to use\n",
    "import quandl\n",
    "config_file = 'eod_test_config.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory is: C:\\personal\\karma_stocks_2021\\stock_investigation\\notebooks\n",
      "path_to_yaml C:\\personal\\karma_stocks_2021\\stock_investigation\\notebooks\\eod_test_config.yml\n"
     ]
    }
   ],
   "source": [
    "# load config file\n",
    "current_path = os.getcwd()\n",
    "print(\"current directory is: \"+current_path)\n",
    "\n",
    "path_to_yaml = os.path.join(current_path, config_file)\n",
    "print(\"path_to_yaml \"+path_to_yaml)\n",
    "try:\n",
    "    with open (path_to_yaml, 'r') as c_file:\n",
    "        config = yaml.safe_load(c_file)\n",
    "except Exception as e:\n",
    "    print('Error reading the config file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.thepythoncode.com/article/using-google-drive--api-in-python\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "# If modifying these scopes, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly']\n",
    "\n",
    "def get_gdrive_service():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    # return Google Drive API service\n",
    "    return(build('drive', 'v3', credentials=creds),creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config parms\n",
    "parms = {}\n",
    "eod_token = config['general']['eod_token']\n",
    "eod_historical_token = config['general']['eod_historical_token']\n",
    "fundamentals_directory = config['files']['fundamentals_directory']\n",
    "\n",
    "from_date = config['general']['master_start']\n",
    "to_date = config['general']['master_end']\n",
    "parms['master_date_mode'] = config['general']['master_date_mode']\n",
    "parms['master_start'] = config['general']['master_start']\n",
    "parms['master_end'] = config['general']['master_end']\n",
    "tse_test_url = config['files']['tse_test']\n",
    "credentials_file = \"credentials.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory is: C:\\personal\\karma_stocks_2021\\stock_investigation\\notebooks\n",
      "path_to_yaml C:\\personal\\karma_stocks_2021\\stock_investigation\\notebooks\\credentials.json\n",
      "credentials is  {'installed': {'client_id': '187319705198-3cij3dcccp544ranf7igrqhcb8nbbb6r.apps.googleusercontent.com', 'project_id': 'transcoder-quickstart-oct10', 'auth_uri': 'https://accounts.google.com/o/oauth2/auth', 'token_uri': 'https://oauth2.googleapis.com/token', 'auth_provider_x509_cert_url': 'https://www.googleapis.com/oauth2/v1/certs', 'client_secret': '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', 'redirect_uris': ['urn:ietf:wg:oauth:2.0:oob', 'http://localhost']}}\n"
     ]
    }
   ],
   "source": [
    "# load credentials token\n",
    "current_path = os.getcwd()\n",
    "print(\"current directory is: \"+current_path)\n",
    "\n",
    "path_to_yaml = os.path.join(current_path, credentials_file)\n",
    "print(\"path_to_yaml \"+path_to_yaml)\n",
    "try:\n",
    "    with open (path_to_yaml, 'r') as c_file:\n",
    "        credentials = yaml.safe_load(c_file)\n",
    "except Exception as e:\n",
    "    print('Error reading the config file')\n",
    "print(\"credentials is \",credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "access_token is  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\n",
      "res is  <Response [401]>\n",
      "{'error': {'errors': [{'domain': 'global', 'reason': 'authError', 'message': 'Invalid Credentials', 'locationType': 'header', 'location': 'Authorization'}], 'code': 401, 'message': 'Invalid Credentials'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nreq = requests.get(URL )\\ntext_data= req.text\\nprint(\"text_data \",text_data)\\njson_dict= json.loads(text_data)\\n\\ndf = pd.DataFrame.from_dict(json_dict[\"Highlights\"])\\ndf.head()\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment to ingest JSON to a dataframe\n",
    "junky, creds = get_gdrive_service()\n",
    "URL = tse_test_url\n",
    "\n",
    "# link of test is https://drive.google.com/file/d/1lAXY00O_tmsoch1SeisEF-EcYEIRyM3j/view?usp=sharing\n",
    "# link of test in native drive: https://drive.google.com/file/d/1hfokSrKEC4fT6qnTk4kFzNHQoOFER8qc/view?usp=sharing\n",
    "\n",
    "#file_id = \"d/1hfokSrKEC4fT6qnTk4kFzNHQoOFER8qc/view\"  # Please set the file ID you want to download.\n",
    "file_id = \"1hfokSrKEC4fT6qnTk4kFzNHQoOFER8qc\"\n",
    "\n",
    "#access_token = credentials['installed'][\"client_secret\"]\n",
    "access_token = creds.token\n",
    "print(\"access_token is \",access_token)\n",
    "#url = \"https://drive.google.com/file/d/1hfokSrKEC4fT6qnTk4kFzNHQoOFER8qc/view?usp=sharing\"\n",
    "url = \"https://www.googleapis.com/drive/v3/files/\" + file_id + \"?alt=media\"\n",
    "res = requests.get(url, headers={\"Authorization\": \"Bearer \" + access_token})\n",
    "print(\"res is \",res)\n",
    "obj = json.loads(res.text)\n",
    "print(obj)\n",
    "\n",
    "'''\n",
    "req = requests.get(URL )\n",
    "text_data= req.text\n",
    "print(\"text_data \",text_data)\n",
    "json_dict= json.loads(text_data)\n",
    "\n",
    "df = pd.DataFrame.from_dict(json_dict[\"Highlights\"])\n",
    "df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataReader: https://riptutorial.com/pandas/topic/1912/pandas-datareader\n",
    "# https://pandas-datareader.readthedocs.io/en/latest/remote_data.html\n",
    "# \n",
    "# Set up End and Start times for data grab\n",
    "# check to see if start and end dates are hard-coded with master dates\n",
    "def set_start_end():\n",
    "    if parms['master_date_mode']: # start and end hardcoded by parameters\n",
    "        start = parms['master_start']\n",
    "        end = parms['master_end']\n",
    "    else: # end is current date; start is current date minus years_window\n",
    "        end = datetime.now()\n",
    "        start = datetime(end.year - parms['years_window'], end.month, end.day)\n",
    "    # output a test dataset\n",
    "    tester = DataReader('IBM', 'stooq', start, end)\n",
    "    print(tester.shape)\n",
    "    return(start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    rawpath = os.getcwd()\n",
    "    # data is in a directory that is a sibling to the directory containing the notebook\n",
    "    path = os.path.abspath(os.path.join(rawpath, '..', fundamentals_directory))\n",
    "    return(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment getting from EOD historical data\n",
    "https://eodhistoricaldata.com/cp/settings\n",
    "https://eodhistoricaldata.com/api/eod/MCD.US?from=2020-01-05&to=2020-02-10&period=d&api_token=6057885c018fd9.09873427\n",
    "\n",
    "- documentation for API https://eodhistoricaldata.com/financial-apis/api-for-historical-data-and-volumes/\n",
    "- https://eodhistoricaldata.com/api/eod/MCD.US?api_token=6057885c018fd9.09873427\n",
    "\n",
    "\n",
    "\n",
    "- MCD.US consists of two parts: {SYMBOL_NAME}.{EXCHANGE_ID}, then you can use, for example, MCD.MX for Mexican Stock Exchange. or MCD.US for NYSE. Check the list of supported exchanges to get more information about stock markets we do support.\n",
    "\n",
    "- api_token – your own API KEY, which you will get after you subscribe to our services.\n",
    "- fmt – the output format. Possible values are ‘csv’ for CSV output and ‘json’ for JSON output. Default value: ‘csv’.\n",
    "- period – use ‘d’ for daily, ‘w’ for weekly, ‘m’ for monthly prices. By default, daily prices will be shown.\n",
    "- order – use ‘a’ for ascending dates (from old to new), ‘d’ for descending dates (from new to old). By default, dates are shown in ascending order.\n",
    "- from and to – the format is ‘YYYY-MM-DD’. If you need data from Jan 5, 2017, to Feb 10, 2017, you should use from=2017-01-05 and to=2017-02-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get data from EOD historical API\n",
    "def get_eod_close_data(symbol='AAPL.US', api_token='OeAFFmMliFG5orCUuwAKQ8l4WWFQ67YX', from_date = '2021-07-01', to_date = '2021-07-08'):\n",
    "    session = requests.Session()\n",
    "    base_url = 'https://eodhistoricaldata.com/api/eod/'+symbol\n",
    "    url = 'https://eodhistoricaldata.com/api/div/%s' % symbol\n",
    "    params = {'api_token': api_token,'from':from_date,'to':to_date}\n",
    "    r = session.get(base_url, params=params)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        df = pd.read_csv(StringIO(r.text), skipfooter=0, parse_dates=[0], index_col=0, engine='python')\n",
    "        return(True, df)\n",
    "    else:\n",
    "        print(\"status code\",str(r.status_code))\n",
    "        print(\"reason code\",str(r.reason))\n",
    "        return(False,\"null\")\n",
    "        #raise Exception(r.status_code, r.reason, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# https://eodhistoricaldata.com/api/fundamentals/AAPL.US?api_token=OeAFFmMliFG5orCUuwAKQ8l4WWFQ67YX\n",
    "def get_eod_ticker_details(symbol='AAPL.US', api_token='OeAFFmMliFG5orCUuwAKQ8l4WWFQ67YX'):\n",
    "    session = requests.Session()\n",
    "    base_url = 'https://eodhistoricaldata.com/api/fundamentals/'+symbol\n",
    "    params = {'api_token': api_token}\n",
    "    r = session.get(base_url, params=params)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        df = pd.read_csv(StringIO(r.text), skipfooter=0,index_col=0)\n",
    "        return(True, r)\n",
    "    else:\n",
    "        print(\"status code\",str(r.status_code))\n",
    "        print(\"reason code\",str(r.reason))\n",
    "        return(False,\"null\")\n",
    "        #raise Exception(r.status_code, r.reason, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the JSON fundamentals dump for a given ticker\n",
    "\n",
    "# https://eodhistoricaldata.com/api/fundamentals/AAPL.US?api_token=OeAFFmMliFG5orCUuwAKQ8l4WWFQ67YX\n",
    "def get_eod_ticker_fundamentals(symbol='AAPL.US', api_token='OeAFFmMliFG5orCUuwAKQ8l4WWFQ67YX'):\n",
    "    session = requests.Session()\n",
    "    base_url = 'https://eodhistoricaldata.com/api/fundamentals/'+symbol\n",
    "    params = {'api_token': api_token}\n",
    "    r = session.get(base_url, params=params)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        df = pd.read_csv(StringIO(r.text), skipfooter=0,index_col=0)\n",
    "        return(True, r)\n",
    "    else:\n",
    "        print(\"status code\",str(r.status_code))\n",
    "        print(\"reason code\",str(r.reason))\n",
    "        return(False,\"null\")\n",
    "        #raise Exception(r.status_code, r.reason, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_file(ticker,output_path,json_struct):\n",
    "    str_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    file_name = os.path.join(output_path,ticker+'_'+str_date+'.json')\n",
    "    print(\"file name is: \",file_name)\n",
    "    #json_object = json.dumps(json_struct, indent = 4)\n",
    "    json_object = json_struct\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(json_object, outfile)\n",
    "    #with open(file_name, \"w\") as outfile:\n",
    "    #    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company is:  W5000.INDX\n",
      "file name is:  C:\\personal\\karma_stocks_2021\\stock_investigation\\static_load_fundamentals\\w5000_2021-10-13.json\n"
     ]
    }
   ],
   "source": [
    "# main block\n",
    "\n",
    "#company = 'IBM.US'\n",
    "#company = 'GSPC.INDX'\n",
    "company = \"W5000.INDX\"\n",
    "token = eod_token\n",
    "print(\"company is: \",company)\n",
    "result_stat, r = get_eod_ticker_fundamentals(company,eod_token)\n",
    "# clean off the backslashes and starting and ending double quotes\n",
    "r_text = r.text\n",
    "json_output = json.loads(r.text)\n",
    "no_backslash = r_text.replace(\"\\\\\", \"\")\n",
    "# my_str[:-1]\n",
    "no_end_quote = no_backslash[:-1]\n",
    "prepped_json = no_end_quote[1:]\n",
    "write_json_file('w5000',get_path(),json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# with open(\"sample.json\", \"w\") as outfile:\\n#    outfile.write(json_object)\\n\\njson_object = json.dumps(r.text, indent = 4)\\nwith open(\"spy.json\", \"w\") as outfile:\\n    outfile.write(json_object)\\n    '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# with open(\"sample.json\", \"w\") as outfile:\n",
    "#    outfile.write(json_object)\n",
    "\n",
    "json_object = json.dumps(r.text, indent = 4)\n",
    "with open(\"spy.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# brute force call \\n# https://eodhistoricaldata.com/api/fundamentals/AAPL.US?api_token=OeAFFmMliFG5orCUuwAKQ8l4WWFQ67YX\\nsession = requests.Session()\\nbase_url = \"https://eodhistoricaldata.com/api/fundamentals/GSPC.INDX\"\\napi_token = \"605766cf49ac93.89487471\"\\nparams = {\\'api_token\\': api_token}\\nr = session.get(base_url, params=params)\\nif r.status_code == requests.codes.ok:\\n    #df = pd.read_csv(StringIO(r.text), skipfooter=0, parse_dates=[0], index_col=0, engine=\\'python\\')\\n    df = pd.read_csv(StringIO(r.text), skipfooter=0,index_col=0)\\n\\n    \\nelse:\\n    print(\"status code\",str(r.status_code))\\n    print(\"reason code\",str(r.reason))\\n    '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# brute force call \n",
    "# https://eodhistoricaldata.com/api/fundamentals/AAPL.US?api_token=OeAFFmMliFG5orCUuwAKQ8l4WWFQ67YX\n",
    "session = requests.Session()\n",
    "base_url = \"https://eodhistoricaldata.com/api/fundamentals/GSPC.INDX\"\n",
    "api_token = \"605766cf49ac93.89487471\"\n",
    "params = {'api_token': api_token}\n",
    "r = session.get(base_url, params=params)\n",
    "if r.status_code == requests.codes.ok:\n",
    "    #df = pd.read_csv(StringIO(r.text), skipfooter=0, parse_dates=[0], index_col=0, engine='python')\n",
    "    df = pd.read_csv(StringIO(r.text), skipfooter=0,index_col=0)\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"status code\",str(r.status_code))\n",
    "    print(\"reason code\",str(r.reason))\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# with open(\"sample.json\", \"w\") as outfile:\\n#    outfile.write(json_object)\\n\\njson_object = json.dumps(r.text, indent = 4)\\nwith open(\"gspc.json\", \"w\") as outfile:\\n    outfile.write(json_object)\\n    '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# with open(\"sample.json\", \"w\") as outfile:\n",
    "#    outfile.write(json_object)\n",
    "\n",
    "json_object = json.dumps(r.text, indent = 4)\n",
    "with open(\"gspc.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
